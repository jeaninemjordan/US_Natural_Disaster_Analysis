# US_Natural_Disaster_Analysis
 
 ## A thorough analyisis studying the correlations and patterns of natural disasters across time and regions in the United States using Python.
 
![](Images/slide1.jpg)

Our team has selected to use FEMA US Disaster Data for all 50 states. The forty years of focus include 1980 - 2020 and details on major disaster events such as:

* Drought
* Fire
* Freezing
* Hurricanes
* Mud/Landslide
* Severe Ice Storm
* Severe Storms
* Snow
* Tsunami
* Volcano

Natural disasters can be catastrophic to the areas that are affected.

Our primary goal in selecting the US Natural Disaster dataset was to observe the overall frequency of major disasters by location, County & State.  This information can be useful when trying to relocate to the United States. Additionally, we would like to model the data to potentially predict events in areas based on past events. 

Thus far, the following has been completed:

### Roles for Deliverable week 1 have been assigned as follows:

* Jeanine - Square: The team member in the square role will be responsible for the repository.
* Deanna - Triangle: The member in the triangle role will create a mockup of a machine learning model. This can even be a diagram that explains how it will work concurrently with the rest of the project steps.
* Kristen - Circle: The member in the circle role will create a mockup of a database with a set of sample data, or even fabricated data. This will ensure the database will work seamlessly with the rest of the project.
* All / Zoe Lackey -X: The member in the X role will decide which technologies will be used for each step of the project.
Presentation - Gilda

### Contact information has been exchanged and a Slack group created for the team to communicate within: 

![](Images/slack.jpg)

### A data set has been selected from Kaggle.com (note: a second data set is being selected to allow for additional metrics to be analyzed):

![](Images/datasetimage.jpg)

### The Github repository has been created with a branch assigned to all team members:

![](Images/branches.jpg)

### The data has been cleaned using Pandas and Jupyter Notebook (see the cleaning.ipynb file in the repository):

![](Images/cleandf.jpg)

### The Google presentation has been created and is in progress:

https://docs.google.com/presentation/d/1F354MDtHzS25DnSC8x3uH112HeP4gVl2OF8Yy9zkmKw/edit?usp=sharing

### A tentative workflow model has been created:

![](Images/workflow.jpg)




